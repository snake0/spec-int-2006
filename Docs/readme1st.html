<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
<head>
<title>Readme 1st CPU2006</title>
<!-- You'll want a nice wide screen when editing this .......................................................................... -->

<link rel="STYLESHEET" href="css/cpu2006docs.css" type="text/css" />
<style type="text/css">
.newterm {font-style:italic; font-weight:bold;}
body { background: white; color: black; font-family: serif;}
</style>
</head>
<body>


<h1>SPEC CPU2006: Read Me First</h1>
<p style="font-size:120%; margin:.1em">Updated for SPEC CPU2006 <span class="new">(new features are highlighted)</span></p>

<p style="margin-top:.1em;font-size:80%">Last updated: 27 Jul 2006  jlh
<br />(To check for possible updates to this document, please see <a
href="http://www.spec.org/cpu2006/Docs/">http://www.spec.org/cpu2006/Docs/</a>.) </p>

<h4 id="intro">Introduction </h4>

<p class="l1">This document provides background information about the SPEC CPU2006 benchmark suite.  SPEC hopes that this
material will help you understand what the benchmark suite can, and cannot, provide; and that it will help you make efficient
use of the product.</p>

<p class="l1">Overall, SPEC designed SPEC CPU2006 to provide a comparative measure of compute intensive performance across
the widest practical range of hardware.  The product consists of source code benchmarks that are developed from real user
applications.  These benchmarks depend on the processor, memory and compiler on the tested system.</p>

<p class="l1">This document is organized as a series of questions and answers.</p>

<p class="l1snugbot">Background</p>
<p class="contentsl2"><a href="#Q1" >    Q1.</a> What is SPEC?                         </p>
<p class="contentsl2"><a href="#Q2" >    Q2.</a> What is a benchmark?                  </p>
<p class="contentsl2"><a href="#Q3" >    Q3.</a> Should I benchmark my own application?</p>
<p class="contentsl2"><a href="#Q4" >    Q4.</a> If not my own application, then what?</p>
<p class="l1snugbot">Scope</p>
<p class="contentsl2"><a href="#Q5" >    Q5.</a> What does SPEC CPU2006 measure?       </p>
<p class="contentsl2"><a href="#Q6" >    Q6.</a> Why use SPEC CPU2006?                 </p>
<p class="contentsl2"><a href="#Q7" >    Q7.</a> What are the limitations of SPEC CPU2006?                          </p>
<p class="l1snugbot">Overview of usage</p>
<p class="contentsl2"><a href="#Q8" >    Q8.</a> What is included in the SPEC CPU2006 package?                      </p>
<p class="contentsl2"><a href="#Q9" >    Q9.</a> What does the user of the SPEC CPU2006 suite have to provide?      </p>
<p class="contentsl2"><a href="#Q10">    Q10.</a> What are the basic steps in running the benchmarks?               </p>
<p class="contentsl2"><a href="#Q11">    Q11.</a> What source code is provided?  What exactly makes up these suites?</p>
<p class="l1snugbot">Metrics</p>
<p class="contentsl2"><a href="#Q12">    Q12.</a> Some of the benchmark names sound familiar; are these comparable to other
programs?  </p>
<p class="contentsl2"><a href="#Q13">    Q13.</a> What metrics can be measured?                                       </p>
<p class="contentsl2"><a href="#Q14">    Q14.</a> What is the difference between a "base" metric and a "peak" metric? </p>
<p class="contentsl2"><a href="#Q15">    Q15.</a> What is the difference between a "rate" and a "speed" metric?       </p>
<p class="contentsl2"><a href="#Q16">    Q16.</a> Which SPEC CPU2006 metric should be used to compare performance?    </p>
<p class="l1snugbot">CPU2006 vs. CPU2000</p>
<p class="contentsl2"><a href="#Q17">    Q17.</a> SPEC CPU2000 is already available. Why create SPEC CPU2006? Will it show
anything different?</p>
<p class="contentsl2"><a href="#Q18">Q18.</a> What happens to SPEC CPU2000 after SPEC CPU2006 is released?</p>
<p class="contentsl2"><a href="#Q19">Q19.</a> Is there a way to translate SPEC CPU2000 results to SPEC CPU2006 results or vice versa?</p>
<p class="l1snugbot">Benchmark selection</p>
<p class="contentsl2"><a href="#Q20"> Q20.</a> What criteria were used to select the benchmarks?</p>
<p class="contentsl2"><a href="#Q21">Q21.</a> Weren&#39;t some of the SPEC CPU2006 benchmarks in SPEC CPU2000? How are they
different?</p>
<p class="contentsl2"><a href="#Q22">Q22.</a> Why were some of the benchmarks not carried over from CPU2000?</p>
<p class="l1snugbot">Miscellaneous</p>
<p class="contentsl2"><a href="#Q23">Q23.</a> Why does SPEC use a reference machine? What machine is used for SPEC CPU2006?</p>
<p class="contentsl2"><a href="#Q24"> Q24.</a> How long does it take to run the SPEC CPU2006 benchmark suites?</p>
<p class="contentsl2"><a href="#Q25"> Q25.</a> What if the tools cannot be run or built on a system? Can the benchmarks be run manually?</p>
<p class="contentsl2"><a href="#Q26">Q26.</a> Where are SPEC CPU2006 results available?</p>
<p class="contentsl2"><a href="#Q27">Q27.</a> Can SPEC CPU2006 results be published outside of the SPEC web site?  Do the
rules still apply?</p>
<p class="contentsl2"><a href="#Q28">Q28.</a> How do I contact SPEC for more information or for technical support?</p>
<p class="contentsl2last"><a href="#Q29">Q29.</a> Now that I have read this document, what should I do next?</p>






<h4 id="Q1">Q1. What is SPEC?</h4>

   <p class="l1">SPEC is the <span class="newterm">S</span>tandard <span 
   class="newterm">P</span>erformance <span class="newterm">E</span>valuation
   <span class="newterm">C</span>orporation.  SPEC is a non-profit 
   organization whose members include computer hardware vendors, software
   companies, universities, research organizations, systems integrators, publishers and consultants.  SPEC's goal is to
   establish, maintain and endorse a standardized set of relevant benchmarks for computer systems.  Although no one set of
   tests can fully characterize overall system performance, SPEC believes that the user community benefits from objective
   tests which can serve as a common reference point.</p>


<h4 id="Q2">Q2. What is a benchmark?</h4>

   <p class="l1"> A <span class="newterm">benchmark</span> is "a standard of measurement or evaluation"
   (Webster&rsquo;s II Dictionary).  A computer benchmark is typically a computer program that performs a strictly defined
   set of operations - a <span class="newterm">workload</span> - and returns some form of result - a <span
   class="newterm">metric</span> - describing how the tested computer performed.  Computer benchmark metrics usually measure
   <span class="newterm">speed</span>: how fast was the workload completed; or <span class="newterm">throughput</span>: how
   many workload units per unit time were completed.  Running the same computer benchmark on multiple computers allows a
   comparison to be made.</p>


<h4 id="Q3">Q3. Should I benchmark my own application?</h4>

   <p class="l1">Ideally, the best comparison test for systems would be your own application with your own workload.
   Unfortunately, it is often impractical to get a wide base of reliable, repeatable and comparable measurements for
   different systems using your own application with your own workload.  Problems might include generation of a good test
   case, confidentiality concerns, difficulty ensuring comparable conditions, time, money, or other constraints.  </p>

<h4 id="Q4">Q4. If not my own application, then what?</h4>

   <p class="l1">You may wish to consider using standardized benchmarks as a reference point.  Ideally, a standardized
   benchmark will be portable, and may already have been run on the platforms that you are interested in.  However,  before
   you consider the results you need to be sure that you understand the correlation between your application/computing needs
   and what the benchmark is measuring.  Are the benchmarks similar to the kinds of applications you run?  Do the workloads
   have similar characteristics?  Based on your answers to these questions, you can begin to see how the benchmark may
   approximate your reality. </p>

   <p class="l1">Note: A standardized benchmark can serve as reference point.  Nevertheless, when you are doing vendor or
   product selection, SPEC does not claim that any standardized benchmark can replace benchmarking your own actual
   application.</p>


<h4 id="Q5">Q5. What does SPEC CPU2006 measure?</h4>

   <p class="l1">SPEC CPU2006 focuses on compute intensive performance, which means these benchmarks emphasize the
   performance of:</p>

    <ul class="l1">
       <li> the computer processor (CPU), </li>
       <li> the memory architecture, and </li>
       <li> the compilers. </li>
    </ul>

   <p class="l1">It is important to remember the contribution of the latter two components.  SPEC CPU performance
   intentionally depends on more than just the processor.</p>

   <p class="l1">SPEC CPU2006 contains two components that focus on two different types of compute intensive performance:</p>

    <ul class="l1">
       <li> The <span class="newterm">CINT2006</span> suite measures compute-intensive integer performance, and </li>
       <li> The <span class="newterm">CFP2006</span> suite measures compute-intensive floating point performance.</li> 
    </ul>

   <p class="l1">SPEC CPU2006 is not intended to stress other computer components such as networking, the operating system,
   graphics, or the I/O system.  For single-CPU tests, the effects from such components on SPEC CPU2006 performance are
   usually minor.  For large <a href="#Q15">rate</a> runs, operating system services may affect performance, and the I/O
   system - number of disks, speed, striping - can have an effect.  Note that there are <a
   href="http://www.spec.org/benchmarks.html">many other SPEC benchmarks</a>, including benchmarks that specifically focus on
   graphics, distributed Java computing, webservers, and network file systems.</p>


<h4 id="Q6">Q6. Why use SPEC CPU2006?</h4>

   <p class="l1">SPEC CPU2006 provides a comparative measure of integer and/or floating point compute intensive performance.
   If this matches with the type of workloads you are interested in, SPEC CPU2006 provides a good reference point.</p>

   <p class="l1">Other advantages to using SPEC CPU2006 include:</p>

    <ul class="l1">
      <li> The benchmark programs are developed from actual end-user applications, as opposed to being synthetic
      benchmarks.</li>
      <li> Multiple vendors use the suite and support it.</li>
      <li> SPEC CPU2006 is highly portable.</li>
      <li> A wide range of results are available at <a href="http://www.spec.org">http://www.spec.org</a></li>
      <li> The benchmarks are required to be run and reported according to a set of rules to ensure comparability and
      repeatability.</li>
    </ul>


<h4 id="Q7">Q7. What are the limitations of SPEC CPU2006?</h4>

   <p class="l1">As <a href="#Q3">described above</a>, the ideal benchmark for vendor or product selection would be your own
   workload on your own application.  Please bear in mind that no standardized benchmark can provide a perfect model of the
   realities of your particular system and user community.</p>


<h4 id="Q8">Q8. What is included in the SPEC CPU2006 package?</h4>

   <p class="l1">SPEC provides the following on the SPEC CPU2006 media (DVD):</p>

    <ul class="l1">
      <li> Source code for the CINT2006 benchmarks</li>
      <li> Source code for the CFP2006 benchmarks</li>
      <li> A tool set for compiling, running, validating and reporting on the benchmarks </li>
      <li> Pre-compiled tools for a variety of operating systems</li>
      <li> Source code for the SPEC CPU2006 tools, for systems not covered by the pre-compiled tools</li>
      <li> Run and reporting rules defining how the benchmarks should be used to produce SPEC CPU2006 results.</li>
      <li> Documentation</li>
    </ul>


<h4 id="Q9">Q9. What does the user of the SPEC CPU2006 suite have to provide?</h4>

   <p class="l1">Briefly, you need a Unix, Linux, Mac OS X,  or Microsoft Windows system with compilers; 8GB of free
   disc space; and a minimum of 1GB of free memory - although more may be required, as described in <a
   href="system-requirements.html">system-requirements.html</a></p>

   <p class="commentarystart" style="margin-top:.5em"> <i>Note:</i> links to SPEC CPU2006 documents on this web page
   assume that you are reading the page from a directory that also contains the other SPEC CPU2006 documents.  If by
   some chance you are reading this web page from a location where the links do not work, try accessing the referenced
   documents at one of the following locations:</p>
   <ul class="commentaryul">
   <li class="commentaryli"><a href="http://www.spec.org/cpu2006/Docs/">www.spec.org/cpu2006/Docs/</a></li>
   <li class="commentaryli">The <span class="ttnobr">$SPEC/Docs/</span> (Unix) or <span class="ttnobr">%SPEC%\Docs\</span>
   (Windows) directory on a system where SPEC CPU2006 has been installed.</li>
   <li class="commentaryli">The <span class="tt">Docs/</span> directory on your SPEC CPU2006 distribution media.</li>
   </ul>
   <p class="commentaryend"> </p>

<h4 id="Q10">Q10. What are the basic steps in running the benchmarks?</h4>

   <p class="l1">Installation and use are covered in detail in the SPEC CPU2006 User Documentation.  The basic steps are:</p> 

     <ul class="l1">
        <li> Ensure that you meet the <a href="system-requirements.html">system requirements</a>.</li>
        <li> Install SPEC CPU2006 from the DVD on <a href="install-guide-unix.html">Unix, Linux, Mac OS X</a>, or <a
        href="install-guide-windows.html">Microsoft Windows</a>. </li>
        <li> Determine which metric you wish to run. </li>
        <li> Learn about <a href="runspec.html">runspec</a>, which is the primary SPEC-provided tool.</li>
        <li> Locate a configuration file as a starting point.  Hints about where to find one are in <a
        href="runspec.html#about_config">runspec.html</a>.</li>
        <li> Use <tt>runspec</tt> to build (compile) the benchmarks.</li>
        <li> If the above steps are successful, use <tt>runspec</tt> to run, validate, and create a report on the performance
        of the benchmarks. </li>
     </ul>

   <p class="l1">If you wish to generate results suitable for quoting in public, you will need to carefully study and adhere
   to the <a href="runrules.html">run rules</a>.</p>


<h4 id="Q11">Q11. What source code is provided?  What exactly makes up these suites?</h4>

   <p class="l1">CINT2006 and CFP2006 are based on compute-intensive applications provided as source code.  CINT2006 contains
   12 benchmarks: 9 use C, and 3 use C++.  The benchmarks are:</p>

<table class="l2">
<tr>
   <td><a href="http://www.spec.org/auto/cpu2006/Docs/400.perlbench.html">400.perlbench</a></td>
   <td>C</td>
   <td>PERL Programming Language      </td>
</tr>
<tr>
   <td><a href="http://www.spec.org/auto/cpu2006/Docs/401.bzip2.html">401.bzip2</a></td>
   <td>C</td>
   <td>Compression     </td>
</tr>
<tr>
   <td><a href="http://www.spec.org/auto/cpu2006/Docs/403.gcc.html">403.gcc</a></td>
   <td>C</td>
   <td>C Compiler        </td>
</tr>
<tr>
   <td><a href="http://www.spec.org/auto/cpu2006/Docs/429.mcf.html">429.mcf</a></td>
   <td>C</td>
   <td>Combinatorial Optimization       </td>
</tr>
<tr>
   <td><a href="http://www.spec.org/auto/cpu2006/Docs/445.gobmk.html">445.gobmk</a></td>
   <td>C</td>
   <td>Artificial Intelligence: go</td>
</tr>
<tr>
   <td><a href="http://www.spec.org/auto/cpu2006/Docs/456.hmmer.html">456.hmmer</a></td>
   <td>C</td>
   <td>Search Gene Sequence</td>
</tr>
<tr>
   <td><a href="http://www.spec.org/auto/cpu2006/Docs/458.sjeng.html">458.sjeng</a></td>
   <td>C</td>
   <td>Artificial Intelligence: chess</td>
</tr>
<tr>
   <td><a href="http://www.spec.org/auto/cpu2006/Docs/462.libquantum.html">462.libquantum</a></td>
   <td>C</td>
   <td>Physics: Quantum Computing       </td>
</tr>
<tr>
   <td><a href="http://www.spec.org/auto/cpu2006/Docs/464.h264ref.html">464.h264ref</a></td>
   <td>C</td>
   <td>Video Compression</td>
</tr>
<tr>
   <td><a href="http://www.spec.org/auto/cpu2006/Docs/471.omnetpp.html">471.omnetpp</a></td>
   <td>C++</td>
   <td>Discrete Event Simulation       </td>
</tr>
<tr>
   <td><a href="http://www.spec.org/auto/cpu2006/Docs/473.astar.html">473.astar</a></td>
   <td>C++</td>
   <td>Path-finding Algorithms </td>
</tr>
<tr>
   <td><a href="http://www.spec.org/auto/cpu2006/Docs/483.xalancbmk.html">483.xalancbmk</a></td>
   <td>C++</td>
   <td>XML Processing</td>
</tr>
</table>

   <p class="l1">CFP2006 has 17 benchmarks:  4 use  C++, 3 use  C, 6 use Fortran, and 4 use a mixture of C and Fortran.  The
   benchmarks are:</p>

<table class="l2">
<tr>
   <td><a href="http://www.spec.org/auto/cpu2006/Docs/410.bwaves.html">410.bwaves</a></td>
   <td>Fortran</td>
   <td>Fluid Dynamics</td>
</tr>
<tr>
   <td><a href="http://www.spec.org/auto/cpu2006/Docs/416.gamess.html">416.gamess</a></td>
   <td>Fortran</td>
   <td>Quantum Chemistry   </td>
</tr>
<tr>
   <td><a href="http://www.spec.org/auto/cpu2006/Docs/433.milc.html">433.milc</a></td>
   <td>C</td>
   <td>Physics: Quantum Chromodynamics</td>
</tr>
<tr>
   <td><a href="http://www.spec.org/auto/cpu2006/Docs/434.zeusmp.html">434.zeusmp</a></td>
   <td>Fortran</td>
   <td>Physics/CFD</td>
</tr>
<tr>
   <td><a href="http://www.spec.org/auto/cpu2006/Docs/435.gromacs.html">435.gromacs</a></td>
   <td>C/Fortran</td>
   <td>Biochemistry/Molecular Dynamics      </td>
</tr>
<tr>
   <td><a href="http://www.spec.org/auto/cpu2006/Docs/436.cactusADM.html">436.cactusADM</a></td>
   <td>C/Fortran</td>
   <td>Physics/General Relativity     </td>
</tr>
<tr>
   <td><a href="http://www.spec.org/auto/cpu2006/Docs/437.leslie3d.html">437.leslie3d</a></td>
   <td>Fortran</td>
   <td>Fluid Dynamics   </td>
</tr>
<tr>
   <td><a href="http://www.spec.org/auto/cpu2006/Docs/444.namd.html">444.namd</a></td>
   <td>C++</td>
   <td>Biology/Molecular Dynamics </td>
</tr>
<tr>
   <td><a href="http://www.spec.org/auto/cpu2006/Docs/447.dealII.html">447.dealII</a></td>
   <td>C++</td>
   <td>Finite Element Analysis</td>
</tr>
<tr>
   <td><a href="http://www.spec.org/auto/cpu2006/Docs/450.soplex.html">450.soplex</a></td>
   <td>C++</td>
   <td>Linear Programming, Optimization</td>
</tr>
<tr>
   <td><a href="http://www.spec.org/auto/cpu2006/Docs/453.povray.html">453.povray</a></td>
   <td>C++</td>
   <td>Image Ray-tracing     </td>
</tr>
<tr>
   <td><a href="http://www.spec.org/auto/cpu2006/Docs/454.calculix.html">454.calculix</a></td>
   <td>C/Fortran</td>
   <td>Structural Mechanics    </td>
</tr>
<tr>
   <td><a href="http://www.spec.org/auto/cpu2006/Docs/459.GemsFDTD.html">459.GemsFDTD</a></td>
   <td>Fortran</td>
   <td>Computational Electromagnetics  </td>
</tr>
<tr>
   <td><a href="http://www.spec.org/auto/cpu2006/Docs/465.tonto.html">465.tonto</a></td>
   <td>Fortran</td>
   <td>Quantum Chemistry       </td>
</tr>
<tr>
   <td><a href="http://www.spec.org/auto/cpu2006/Docs/470.lbm.html">470.lbm</a></td>
   <td>C</td>
   <td>Fluid Dynamics  </td>
</tr>
<tr>
   <td><a href="http://www.spec.org/auto/cpu2006/Docs/481.wrf.html">481.wrf</a></td>
   <td>C/Fortran</td>
   <td>Weather Prediction  </td>
</tr>
<tr>
   <td><a href="http://www.spec.org/auto/cpu2006/Docs/482.sphinx3.html">482.sphinx3</a></td>
   <td>C</td>
   <td>Speech recognition      </td>
</tr>
</table>

   <p class="l1">Descriptions of the benchmarks, with reference to papers, web sites, and so forth, can be found in the
   individual benchmark descriptions (click the links above).  Some of the benchmarks also provide additional details, such
   as documentation from the original program, in the nnn.benchmark/Docs directories in the SPEC benchmark tree. </p>

   <p class="l1">The numbers used as part of the benchmark names provide an identifier to help distinguish programs from one
   another.  For example, some programs were updated from <a href="http://www.spec.org/cpu2000/">SPEC CPU2000</a> and need to
   be distinguished from the previous version.  Note: even if a program has the same name as in a previous suite - for
   example, 176.gcc vs. 403.gcc - the updated workload and updated source code mean that it is not valid to compare SPEC
   CPU2006 results to results with older SPEC CPU benchmarks.</p>

<h4 id="Q12">Q12. Some of the benchmark names sound familiar; are these comparable to other programs?</h4>

   <p class="l1">Many of the SPEC benchmarks have been derived from publicly available application programs.
   The individual benchmarks in this suite may be similar, but are NOT identical to benchmarks or programs
   with similar names which may be available from sources other than SPEC.   In particular, SPEC has 
   invested significant effort to improve portability and to minimize hardware dependencies, 
   to avoid unfairly favoring one hardware platform over another.  For this reason, the application programs in
   this distribution may perform differently from commercially available versions of the
   same application.  </p>

   <p class="l1">Therefore, it is not valid to compare SPEC CPU2006 benchmark results with anything other than other SPEC
   CPU2006 benchmark results.  </p>


<h4 id="Q13">Q13. What metrics can be measured?</h4>

   <p class="l1">After the benchmarks are run on the system under test (SUT), a ratio for each of them is calculated using
   the run time on the SUT and a SPEC-determined <a href="#Q23">reference time</a>.  From these ratios, the following metrics
   are calculated:</p>

   <p class="l1">CINT2006 (for integer compute intensive performance comparisons): </p>

    <ul class="l1">
        <li> <span class="newterm">SPECint2006</span>: The geometric mean of twelve normalized ratios - one for each integer
        benchmark - when the benchmarks are compiled with <a href="#Q14">peak</a> tuning.</li>
        <li> <span class="newterm">SPECint_base2006</span>: The geometric mean of twelve normalized ratios when the
        benchmarks are compiled with <a href="#Q14">base</a> tuning.</li>
        <li> <span class="newterm">SPECint_rate2006</span>: The geometric mean of twelve normalized <a
        href="#Q15">throughput</a> ratios when the benchmarks are compiled with peak tuning.</li>
        <li> <span class="newterm">SPECint_rate_base2006</span>: The geometric mean of twelve normalized throughput ratios
        when the benchmarks are compiled with base tuning.</li>
    </ul>

   <p class="l1">CFP2006 (for floating point compute intensive performance comparisons: </p>

    <ul class="l1">
        <li> <span class="newterm">SPECfp2006</span>: The geometric mean of seventeen normalized ratios - one for each
        floating point benchmark - when compiled with <a href="#Q14">peak</a> tuning.</li>
        <li> <span class="newterm">SPECfp_base2006</span>: The geometric mean of seventeen normalized ratios when the
        benchmarks are compiled with <a href="#Q14">base</a> tuning.</li>
        <li> <span class="newterm">SPECfp_rate2006</span>: The geometric mean of seventeen normalized <a
        href="#Q15">throughput</a> ratios when the benchmarks are compiled with peak tuning.</li>
        <li> <span class="newterm">SPECfp_rate_base2006</span>: The geometric mean of seventeen normalized throughput ratios
        when the benchmarks are compiled with base tuning.</li>
     </ul>

   <p class="l1">In all cases, a higher score means "better performance" on the given workload.</p>


<h4 id="Q14">Q14. What is the difference between a "base" metric and a "peak" metric?</h4>

   <p class="l1">In order to provide comparisons across different computer hardware, SPEC provides the benchmarks as source
   code.  Thus, in order to run the benchmarks, they must be compiled.  There is agreement that the benchmarks should be
   compiled the way users compile programs.  But how do users compile programs? </p>

   <ul class="l1">
      <li><p>Some users might experiment with many different compilers and compiler flags to achieve the best performance,
      and may be willing to develop multi-step make processes and "training" workloads.</p></li> 

      <li><p>Other users might prefer the relative simplicity of using a single set of switches and a single-step make
      process. </p></li>
   </ul>

   <p class="l1">In addition to the above, a wide range of other types of usage models could also be imagined, ranging in a
   continuum from <tt>-Odebug</tt> at the low end, to inserting directives and/or re-writing the source code at the high end.
   Which points on this continuum should SPEC CPU2006 allow?</p>

   <p class="l1">SPEC recognizes that any point chosen from that continuum might seem arbitrary to those whose interests lie
   at a different point.  Nevertheless, choices must be made.</p>

   <p class="l1">For CPU2006, SPEC has chosen to allow two types of compilation:</p>

   <ul class="l1">
        <li><p> The <span class="newterm">base</span> metrics (e.g. SPECint_base2006) are required for all reported results
        and have stricter guidelines for compilation.  For example, the same flags must be used in the same order for all
        benchmarks of a given language.  This is the point closer to those who might prefer a relatively simple build
        process.</p></li>

        <li><p> The <span class="newterm">peak</span> metrics (e.g. SPECint2006) are optional and have less strict
        requirements.  For example, different compiler options may be used on each benchmark, and feedback-directed
        optimization is allowed.  This point is closer to those who may be willing to invest more time and effort in
        development of build procedures.</p></li>
    </ul>

   <p class="l1">Note that options allowed under the base metric rules are a subset of those allowed under the peak metric
   rules.  A legal base result is also legal under the peak rules but a legal peak result is NOT necessarily legal under the
   base rules.</p>

   <p class="l1">A full description of the distinctions and required guidelines can be found in the <a
   href="runrules.html">SPEC CPU2006 Run and Reporting Rules</a>.</p>


<h4 id="Q15">Q15. What is the difference between a "rate" and a "speed" metric?</h4>

   <p class="l1">There are several different ways to measure computer performance.  One way is to measure how fast the
   computer completes a single task; this is a <span class="newterm">speed</span> measure.  Another way is to measure how
   many tasks a computer can accomplish in a certain amount of time; this is called a throughput, capacity or <span
   class="newterm">rate</span> measure. </p>

    <ul class="l1">
        <li> The SPEC speed metrics (e.g., SPECint2006) are used for comparing the ability of a computer to complete single
        tasks. </li>
        <li> The SPEC rate metrics (e.g., SPECint_rate2006) measure the throughput or rate of a machine carrying out a number
        of tasks. </li>
    </ul>

   <p class="l1">For the rate metrics, multiple copies of the benchmarks are run simultaneously.  Typically, the number of
   copies is the same as the number of CPUs on the machine, but this is not a requirement.  For example, it would be
   perfectly acceptable to run 63 copies of the benchmarks on a 64-CPU machine (thereby leaving one CPU free to handle system
   overhead).</p>

   <p class="l1">Note: a speed run which uses a parallelizing compiler to distribute one copy of a benchmark over multiple
   CPUs is still a speed run, and uses the speed metrics.  You can identify such runs by the field "Auto Parallel".</p>


<h4 id="Q16">Q16. Which SPEC CPU2006 metric should be used to compare performance?</h4>

   <p class="l1">It depends on your needs. SPEC provides the benchmarks and results as tools for you to use.  You need to
   determine how you use a computer or what your performance requirements are and then choose the appropriate SPEC benchmark
   or metrics.  </p>

   <p class="l1">A single user running a compute-intensive integer program, for example,  might only be interested in
   SPECint2006 or SPECint_base2006.  On the other hand, a person who maintains a machine used by multiple scientists running
   floating point simulations might be more concerned with SPECfp_rate2006 or SPECfp_rate_base2006. </p>

<h4 id="Q17">Q17: SPEC CPU2000 is already available. Why create SPEC CPU2006? Will it show anything different?</h4>

   <p class="l1">Technology is always improving. As the technology improves, the benchmarks should improve as well. SPEC
   needed to address the following issues:</p> 

   <div class="l2">

      <p> <b>Run-time:</b>
      <br /> As of summer, 2006, many of the CPU2000 benchmarks are finishing in less than a minute on leading-edge
      processors/systems.  Small changes or fluctuations in system state or measurement conditions can therefore have
      significant impacts on the percentage of observed run time.  SPEC chose to make run times for CPU2006 benchmarks longer
      to take into account future performance and prevent this from being an issue for the lifetime of the suites.  </p> 

     <p> <b>Application size:</b>
     <br /> As applications grow in complexity and size, CPU2000 becomes less representative of what runs on current systems.
     For CPU2006, SPEC included some programs with both larger resource requirements and more complex source code.  </p>

     <p> <b>Application type:</b>
     <br /> SPEC felt that there were additional application areas that should be included in CPU2006 to increase variety and
     representation within the suites.  For example, video compression and speech recognition have been added, and molecular
     biology has been significantly expanded.  </p>

     <p> <b>Moving target:</b>
     <br /> CPU2000 has been available for six years and much improvement in hardware and software has occurred during this
     time. Benchmarks need to evolve to keep pace with improvements.  </p>

  </div>

<h4 id="Q18">Q18: What happens to SPEC CPU2000 after SPEC CPU2006 is released?</h4>

   <p class="l1">SPEC will begin the process of retiring CPU2000. Three months after the announcement of CPU2006, SPEC
   will require all CPU2000 results submitted for publication on SPEC's web site to be accompanied by CPU2006 results. Six
   months after announcement, SPEC will stop accepting CPU2000 results for publication on its web site. </p>

<h4 id="Q19">Q19: Is there a way to translate SPEC CPU2000 results to SPEC CPU2006 results or vice versa?</h4>

   <p class="l1">There is no formula for converting CPU2000 results to CPU2006 results and vice versa; they are
   different products. There probably will be some correlation between CPU2000 and CPU2006 results (i.e., machines with
   higher CPU2000 results often will have higher CPU2006 results), but there is no universal formula for all systems.</p> 

   <p class="l1"> SPEC encourages SPEC licensees to publish CPU2006 numbers on older platforms to provide a historical
   perspective on performance.  </p>

<h4 id="Q20"> Q20: What criteria were used to select the benchmarks?</h4>

   <p class="l1">In the process of selecting applications to use as benchmarks, SPEC considered the following
   criteria:</p> 
        <ul class="l1">
          <li> portability to a variety of CPU architectures (32- and 64-bit including AMD64, Intel IA32, Itanium, 
          PA-RISC, PowerPC, SPARC, etc.) </li>
          <li> portability to various operating systems, particularly UNIX and Windows </li>
          <li> nearly all of the time is spent compute bound </li>
          <li> little time spent in IO and system services</li>
          <li> benchmarks should run in about 1GB RAM without swapping or paging </li>
          <li> no more than five percent of benchmarking time should be spent processing code not provided by SPEC</li>
          <li> well-known applications or application areas</li>
          <li> available workloads that represent real problems</li>
        </ul>

<h4 id="Q21">Q21: Weren&#39;t some of the SPEC CPU2006 benchmarks in SPEC CPU2000? How are they different?</h4>

   <p class="l1">Although some of the benchmarks from CPU2000 are included in CPU2006, they all have been given different
   workloads and/or modified to use newer versions of the source code.  Therefore, for example, results with the CPU2000
   benchmark 181.mcf may be strikingly different from results with the CPU2006 benchmark 429.mcf.</p>

<h4 id="Q22">Q22: Why were some of the benchmarks not carried over from CPU2000?</h4>

   <p class="l1">Some benchmarks were not retained because it was not possible to create a longer-running or more robust
   workload. Others were left out because SPEC felt that they did not add significant performance information compared to the
   other benchmarks under consideration.</p>

<h4 id="Q23">Q23: Why does SPEC use a reference machine? What machine is used for SPEC CPU2006?</h4>

   <p class="l1">SPEC uses a reference machine to normalize the performance metrics used in the CPU2006 suites. Each
   benchmark is run and measured on this machine to establish a reference time for that benchmark. These times are then used
   in the SPEC calculations.</p> 

   <p class="l1">SPEC uses a historical Sun system, the "Ultra Enterprise 2" which was introduced in 1997, as the reference
   machine.  The reference machine uses a 296 MHz UltraSPARC II processor, as did the reference machine for CPU2000.  But the
   reference machines for the two suites are not identical: the CPU2006 reference machine has substantially better caches,
   and the CPU2000 reference machine could not have held enough memory to run CPU2006.</p>
   
   <p class="l1">It takes about 12 days to do a rule-conforming run of the base metrics for CINT2006 and CFP2006 on the
   CPU2006 reference machine.  </p>

   <p class="l1"> Note that when comparing any two two systems measured with the CPU2006, their performance relative to each
   other would remain the same even if a different reference machine was used. This is a consequence of the mathematics
   involved in calculating the individual and overall (geometric mean) metrics.  </p>

<h4 id="Q24"> Q24: How long does it take to run the SPEC CPU2006 benchmark suites?</h4>

   <p class="l1">This depends on the suite and the machine that is running the benchmarks. As mentioned above, the reference
   (historical) machine takes on the order of 12 days; contemporary machines might take on the order of a couple days.
   Again, though, it depends on which metrics are run.</p>

<h4 id="Q25"> Q25: What if the tools cannot be run or built on a system? Can the benchmarks be run manually?</h4>

   <p class="l1">To generate rule-compliant results, an approved toolset must be used.  If several attempts at using the
   SPEC-provided tools are not successful, you should <a href="#Q28">contact SPEC for technical support</a>. SPEC may be able
   to help you, but this is not always possible -- for example, if you are attempting to build the tools on a platform that
   is not available to SPEC.</p>

   <p class="l1">If you just want to work with the benchmarks and do not care to generate publishable results, SPEC
   provides information about <a href="runspec-avoidance.html">how to do so</a>.</p>

<h4 id="Q26">Q26: Where are SPEC CPU2006 results available?</h4>

   <p class="l1">Results for measurements submitted to SPEC are available at <a
   href="http://www.spec.org/cpu2006/">http://www.spec.org/cpu2006</a>.</p>

<h4 id="Q27">Q27: Can SPEC CPU2006 results be published outside of the SPEC web site?  Do the rules still apply?</h4>

   <p class="l1">Yes, SPEC CPU2006 results can be freely published if all the run and reporting rules have been followed. The
   CPU2006 license agreement binds every purchaser of the suite to the run and reporting rules if results are quoted in
   public. A full disclosure of the details of a performance measurement must be provided on request. </p> 

   <p class="l1"> SPEC strongly encourages that results be submitted for publication on <a
   href="http://www.spec.org/cpu2006">SPEC's web site</a>, since it ensures a peer review process and uniform presentation of
   all results.  </p>

   <p class="l1"> The run and reporting <a href="runrules.html#rule_4.5">rules for research and and academic contexts</a>
   recognize that it may not be practical to comply with the full set of rules in some contexts.  It is always required,
   however, that non-compliant results must be clearly distinguished from rule-compliant results.</p>


<h4 id="Q28">Q28. How do I contact SPEC for more information or for technical support?</h4>

   <p class="l1">SPEC can be contacted in several ways. For general information, including other means of contacting SPEC,
   please see SPEC's Web Site at:</p>

   <p class="l2"><a href="http://www.spec.org/"><b>http://www.spec.org/</b></a></p>

    <p class="l1">General questions can be emailed to: 
          <a href="&#109;ailto&#58;info&#64;spec.&#111;rg">info&#64;spec.&#111;rg</a>

    <br />CPU2006 Technical Support Questions can be sent to: 
          <a href="&#109;ailto&#58;cpu2006support&#64;spec.&#111;rg">cpu2006support&#64;spec.&#111;rg</a></p>


<h4 id="Q29">Q29. Now that I have read this document, what should I do next?</h4>
   
   <p class="l1">If you haven't bought CPU2006, it is hoped that you will consider doing so.  If you are ready to get started
   using the suite, then you should pick a system that meets the requirements as described in </p>

   <p class="l2"><a href="system-requirements.html">system-requirements.html</a></p>

   <p class="l1">and install the suite, following the instructions in</p>
       
   <p class="l2"><a href="install-guide-unix.html">install-guide-unix.html</a> or 
   <br /><a href="install-guide-windows.html">install-guide-windows.html</a></p>

<p>Questions and answers were prepared by <a href="http://www.spec.org/spec/kaivalya/">Kaivalya Dixit</a> of IBM, Jeff Reilly
of Intel Corp, and John Henning of Sun Microsystems. Dixit was the long-time President of SPEC, Reilly is Chair of the SPEC
CPU Subcommittee, and Henning is Vice-Chair/Secretary of the SPEC CPU Subcommittee.</p>

<hr />
<p>Copyright (C) 1995-2006 Standard Performance Evaluation Corporation
<br />All Rights Reserved</p>
</body>
</html>

